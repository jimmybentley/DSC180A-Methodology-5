James Bentley, jbentley@ucsd.edu

B07, Lindsey Kostas

1) The applications of graph ML on integrated circuits for different down-stream tasks was the most interesting part. 

2) Finding representative batches of subgraphs for optimal batch processing to reduce training time and compute resources used.

3) We currently are considering partition and centrality algorithms for creating representations. I am considering trying out contrastive learning and curriculum learning.

4) We could try training on multiple gpus to check out how our model scales on multiple gpu partitions.
